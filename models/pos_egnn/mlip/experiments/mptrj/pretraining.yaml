general:
  seed: 2198655
  profiler: False
  debug_mode: False
  
dataset:
  # PROCESSING
  n_jobs: 1
  cutoff: 6.0
  reprocess: False
  
  # TRAINING
  batch_size: 6 # 10
  datasets:
    MPtrj: ["Snapshot"]
    # ANI2X: ["Snapshot"]
  split:
    seed: 2198655
    train: 0.9
    val: 0.1
  transforms: []
  dataloader_n_workers: 8

encoder:
  arch: "Geodite"
  args:
    hidden_channels: 192
    num_layers: 4
    radial_basis: besselbasis
    trainable_radial: False
    n_rbf: 12
    cutoff: 6.0
    activation: silu
    max_z: 100
    epsilon: 1.0e-6
    weight_init: xavier_uniform
    num_heads: 8
    attn_dropout: 0.2
    edge_updates: linw_mlpa_ln
    lmax: 2
    aggr: add
    evec_dim: 192
    emlp_dim: 192
    distance_weighting: True
    distance_weighting_trainable: True

decoder:
  snapshot_weights: 
    - 1
    - 1
    - 1

optimizer:
  method: "AdamW"
  args:
    lr: 1.0e-4
    weight_decay: 0.01
    betas:
      - 0.90
      - 0.999
  scalarization:
    method: "Sum"
    args: {}
  schedulers:
    - method: "WarmupLinearDecayLR"
      method_args:
        warmup_iters: 2000
        T_max: 800000
        eta_max: 4.5e-4
        eta_min: 0.0
      monitor_args:
        interval: step
        frequency: 1

trainer:
  max_epochs: 200
  accelerator: "cuda"
  log_every_n_steps: 500
  devices: 8
  num_nodes: 2
  strategy: "ddp"
  gradient_clip_val: 0.0
  precision: 32

callbacks:
  - name: zclip
    args:
      alpha: 0.97
      z_thresh: 2.5
  - name: summary
    args:
      max_depth: 4
  - name: ema
    args:
      decay: 0.995
      every_n_steps: 1
  - name: lr_monitor
    args:
      logging_interval: step
  - name: checkpoint
    args:
      filename: "best-checkpoint-{epoch:02d}"
      save_top_k: 10
      save_last: False
      every_n_epochs: 1
      monitor: "Total loss/Validation"
      mode: min
  - name: checkpoint
    args:
      save_top_k: 0
      save_last: True
      every_n_train_steps: 250